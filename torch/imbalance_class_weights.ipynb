{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Howl06/practice/blob/main/imbalance_class_weights.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXUeRLKrvfyu",
        "outputId": "59ae05d2-2161-4173-98d8-72dac6f0ac0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchsampler\n",
            "  Downloading torchsampler-0.1.2-py3-none-any.whl (5.6 kB)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.9/dist-packages (from torchsampler) (2.0.0+cu118)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from torchsampler) (1.5.3)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.9/dist-packages (from torchsampler) (0.15.1+cu118)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.3->torchsampler) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.3->torchsampler) (2.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.3->torchsampler) (3.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.3->torchsampler) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.3->torchsampler) (1.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.3->torchsampler) (3.11.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.3->torchsampler) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.3->torchsampler) (3.25.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision>=0.5->torchsampler) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision>=0.5->torchsampler) (8.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision>=0.5->torchsampler) (2.27.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->torchsampler) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->torchsampler) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->torchsampler) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.3->torchsampler) (2.1.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.5->torchsampler) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.5->torchsampler) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.5->torchsampler) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.5->torchsampler) (2.0.12)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.3->torchsampler) (1.3.0)\n",
            "Installing collected packages: torchsampler\n",
            "Successfully installed torchsampler-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install torchsampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BGjXQ9SngUl",
        "outputId": "bd6a3529-afe0-468a-9012-b4014c7d647c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using 2 dataloader workers every process\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "\n",
        "from torch import Tensor\n",
        "from typing import List\n",
        "from torchsampler import ImbalancedDatasetSampler\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "hyper_params = {\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"epochs\": 10,\n",
        "    \"batch_size\": 2,\n",
        "    \"num_classes\": 27,\n",
        "    \"alpha\": 1,\n",
        "    \"input_size\": 224,\n",
        "    \"loss_function_name\": nn.CrossEntropyLoss(),\n",
        "    \"optimizer_name\": optim.Adam\n",
        "    }\n",
        "    \n",
        "\n",
        "# path\n",
        "classjson_path = \"/content/drive/MyDrive/data_set/class_indices.json\"\n",
        "root_path = \"/content/drive/MyDrive\"\n",
        "data_set_dir = \"data_set\"\n",
        "flower_dir = \"project_data\"\n",
        "weight_path = \"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ToTensor H，W，C ——> C，H，W C/255 [0~1]\n",
        "data_transform = {\n",
        "    \"train\": transforms.Compose([transforms.RandomResizedCrop(224),\n",
        "                                  transforms.RandomHorizontalFlip(),\n",
        "                                  transforms.ToTensor(),\n",
        "                                  transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
        "    \"val\": transforms.Compose([transforms.Resize(256),\n",
        "                                transforms.CenterCrop(224),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
        "    \"test\": transforms.Compose([transforms.Resize(256),\n",
        "                                transforms.CenterCrop(224),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])}\n",
        "\n",
        "data_root = os.path.abspath(os.path.join(os.getcwd(), root_path))  # get data root path\n",
        "image_path = os.path.join(data_root, data_set_dir, flower_dir)  # flower data set path\n",
        "\n",
        "assert os.path.exists(image_path), \"{} path does not exist.\".format(image_path)\n",
        "train_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"train\"),\n",
        "                                      transform=data_transform[\"train\"])\n",
        "train_num = len(train_dataset)\n",
        "\n",
        "# {'daisy':0, 'dandelion':1, 'roses':2, 'sunflower':3, 'tulips':4}\n",
        "flower_list = train_dataset.class_to_idx\n",
        "cla_dict = dict((val, key) for key, val in flower_list.items())\n",
        "\n",
        "\n",
        "# write dict into json file\n",
        "json_str = json.dumps(cla_dict, indent=4)\n",
        "with open(classjson_path, 'w') as json_file:\n",
        "    json_file.write(json_str)\n",
        "\n",
        "nw = min([os.cpu_count(), hyper_params[\"batch_size\"] if hyper_params[\"batch_size\"] > 1 else 0, 8])  # number of workers\n",
        "print('Using {} dataloader workers every process'.format(nw))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                        #sampler=ImbalancedDatasetSampler(train_dataset),\n",
        "                                        shuffle=True,\n",
        "                                        batch_size=hyper_params[\"batch_size\"],\n",
        "                                        num_workers=nw)\n",
        "\n",
        "validate_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"val\"),\n",
        "                                        transform=data_transform[\"val\"])\n",
        "val_num = len(validate_dataset)\n",
        "validate_loader = torch.utils.data.DataLoader(validate_dataset,\n",
        "                                              batch_size=hyper_params[\"batch_size\"], shuffle=False,\n",
        "                                              num_workers=nw)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9hfWzpcO-Zn",
        "outputId": "032c8cff-7084-4d78-bb8a-799e9baa72a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset ImageFolder\n",
              "    Number of datapoints: 68\n",
              "    Root location: /content/drive/MyDrive/data_set/project_data/train\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=warn)\n",
              "               RandomHorizontalFlip(p=0.5)\n",
              "               ToTensor()\n",
              "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "           )"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "Gx8moZ8xk4eS",
        "outputId": "f00e79e6-8749-4ef7-d728-6f1f18038e94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using cpu device.\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-1aa374c6ec62>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mclass_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mloss_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyper_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss_function_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'weight'"
          ]
        }
      ],
      "source": [
        "\n",
        "from collections import Counter \n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"using {} device.\".format(device))\n",
        "label_nums_dic = Counter([j for i, j in train_dataset.imgs])\n",
        "sample_num_list = label_nums_dic.values()\n",
        "max_sample_num = max(label_nums_dic.values())\n",
        "weights = [max_sample_num/label_nums for label_nums in sample_num_list]\n",
        "class_weights = torch.FloatTensor(weights).to(device)\n",
        "loss_function = hyper_params[\"loss_function_name\"](weight=class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyROznGGIVm9"
      },
      "outputs": [],
      "source": [
        "reback_img = transforms.Compose([\n",
        "                transforms.Normalize(\n",
        "                mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
        "                std=[1/0.229, 1/0.224, 1/0.225])]\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "S_FmnFikEbiR"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "train_bar = tqdm(train_loader, file=sys.stdout)\n",
        "for step, data in enumerate(train_bar):\n",
        "    train_images, labels = data\n",
        "    train_images = reback_img(train_images)\n",
        "    for pre_index, image in enumerate(train_images):\n",
        "      img = transforms.ToPILImage()(image)\n",
        "      img.show()\n",
        "      img = np.array(img, dtype=np.uint8)\n",
        "      print(img.shape)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1y-aHzakLJTZDlBFSOA-y5F5g2IrrGhxJ",
      "authorship_tag": "ABX9TyNHnpUp+AEs4E9heNln8M6b",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}